# -*- coding: utf-8 -*-
"""Recipe_RAG_BOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JeylpnwsRI5U5woQoQU-_cO15Be3iPng
"""

pip install cohere PyPDF2 numpy

import cohere
from PyPDF2 import PdfReader
from typing import List
import numpy as np
import re

# --- Initialize Cohere Client ---
COHERE_API_KEY = "TjRlGT2z3C8nqBlNysvvtWYFBw3ynV2ylYDobuUA"
co = cohere.Client(COHERE_API_KEY)

# --- Load and Chunk PDF ---
def load_pdf_chunks(pdf_path: str, chunk_size: int = 300) -> List[str]:
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text + "\n"
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

# --- Embed Chunks with Document Type ---
def embed_chunks(chunks: List[str]):
    response = co.embed(
        texts=chunks,
        model="embed-english-v3.0",
        input_type="search_document"
    )
    return response.embeddings

# --- Embed Query and Retrieve Top Chunks ---
def retrieve_chunks(query: str, chunks: List[str], embeddings, top_k: int = 3):
    query_emb = co.embed(
        texts=[query],
        model="embed-english-v3.0",
        input_type="search_query"
    ).embeddings[0]

    similarities = [
        np.dot(query_emb, emb) / (np.linalg.norm(query_emb) * np.linalg.norm(emb))
        for emb in embeddings
    ]
    top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:top_k]
    return [chunks[i] for i in top_indices]

# --- Web Search Fallback Using Connector ---
def cohere_connector_search(query: str) -> str:
    connector_response = co.chat(
        message=query,
        connectors=[{"id": "web-search"}],
        model="command-r-plus",
        temperature=0.4
    )
    return connector_response.text

# --- Generate Answer from Context ---
def generate_answer(query: str, context: str):
    prompt = (
        f"Answer the user's question using the provided context. If the context is not relevant, say you donâ€™t know.\n\n"
        f"Context:\n{context}\n\n"
        f"Question: {query}\n\n"
        f"Answer:"
    )
    response = co.generate(
        model="command-r-plus",
        prompt=prompt,
        max_tokens=500,
        temperature=0.4,
    )
    return response.generations[0].text.strip()

# --- Simple Query Validation ---
def validate_query(query: str) -> bool:
    # Check query length
    if len(query) < 3:
        print("âš ï¸ Query too short. Please enter a more detailed question.")
        return False
    # Check for non-printable characters or gibberish using regex (basic)
    if not re.match(r'^[\w\s,?.!\'"-]+$', query):
        print("âš ï¸ Query contains invalid characters. Please use letters, numbers, and common punctuation only.")
        return False
    return True

# --- Full Handler ---
def handle_query(query: str, pdf_path: str):
    if not validate_query(query):
        return "Invalid query. Please try again with a proper question."

    chunks = load_pdf_chunks(pdf_path)
    embeddings = embed_chunks(chunks)
    top_chunks = retrieve_chunks(query, chunks, embeddings)
    context = "\n".join(top_chunks)
    answer = generate_answer(query, context)

    if "donâ€™t know" in answer.lower() or len(answer.strip()) < 20:
        print("ðŸ“¡ Answer not found in PDF. Using web search...")
        answer = cohere_connector_search(query)

    return answer

# --- Run Chatbot ---
if __name__ == "__main__":
    pdf_path = "/content/Recipe Book.pdf"  # Update this to your PDF path
    while True:
        user_query = input("Ask a question (type 'exit' to quit): ").strip()
        if user_query.lower() in ['exit', 'quit']:
            break
        response = handle_query(user_query, pdf_path)
        print("\nðŸ¤– Assistant:\n", response)